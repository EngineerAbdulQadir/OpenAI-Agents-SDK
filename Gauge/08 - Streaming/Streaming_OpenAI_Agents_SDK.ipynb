{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install openai-agents SDK"
      ],
      "metadata": {
        "id": "PdKwzEluDBN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents"
      ],
      "metadata": {
        "id": "3QdkOviEB2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fbe385-f67e-45a7-c748-29ee7c509f2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "7yD91lz4DIAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "K3VTUWDaGFcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "from agents import (\n",
        "    Agent,\n",
        "    Runner,\n",
        "    set_default_openai_api,\n",
        "    set_default_openai_client,\n",
        "    set_tracing_disabled,\n",
        ")\n",
        "\n",
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY_1\")\n",
        "\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "set_default_openai_client(client=external_client, use_for_tracing=False)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "set_tracing_disabled(disabled=True)"
      ],
      "metadata": {
        "id": "QSIWS6RvC-a4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Text code\n",
        "\n",
        "Here’s what’s happening, line by line:\n",
        "\n",
        "```python\n",
        "result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "```\n",
        "\n",
        "1. **`Runner.run_streamed(...)`**  \n",
        "   - Instead of running the agent to completion and returning a single final string, this kicks off a *streaming* run.  \n",
        "   - You pass in your `agent` instance and the user prompt (`\"Please tell me 5 jokes.\"`).  \n",
        "   - It immediately returns a `StreamedRunResult` (let’s call it `result`) which you can asynchronously iterate to receive partial outputs as they arrive.\n",
        "\n",
        "```python\n",
        "async for event in result.stream_events():\n",
        "```\n",
        "\n",
        "2. **`async for`**  \n",
        "   - This is an asynchronous loop over `result.stream_events()`, which yields every event emitted during the run.  \n",
        "   - Events can include things like “token received,” “tool call started,” “tool call finished,” and so on.\n",
        "\n",
        "```python\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "```\n",
        "\n",
        "3. **Filtering for text deltas**  \n",
        "   - Each `event` has a `.type` (a simple string) and a `.data` payload (an object with more details).  \n",
        "   - We check two things:  \n",
        "     1. `event.type == \"raw_response_event\"`: this means it’s part of the model’s chat response, not a tool call or metadata.  \n",
        "     2. `isinstance(event.data, ResponseTextDeltaEvent)`: among response events, we further narrow to those carrying *text delta* updates (i.e. snippets of the assistant’s reply as it’s generated).\n",
        "\n",
        "```python\n",
        "        print(event.data.delta, end=\"\", flush=True)\n",
        "```\n",
        "\n",
        "4. **Printing out the partial text**  \n",
        "   - Each `ResponseTextDeltaEvent` has a `.delta` property: a string fragment (often a word or sub-word piece) that the model just produced.  \n",
        "   - By doing `print(..., end=\"\", flush=True)`, you:  \n",
        "     - Append that fragment to the console without adding a newline.  \n",
        "     - `flush=True` forces Python to write it out immediately, so you see the text appear in real time as it streams in.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FXBrF52IPBM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg583lQEBRWo",
        "outputId": "0191c635-32a7-47e5-c8dc-cb8b388409ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, here are 5 jokes for you:\n",
            "\n",
            "1.  Why don't scientists trust atoms?\n",
            "    Because they make up everything!\n",
            "\n",
            "2.  Parallel lines have so much in common.\n",
            "    It's a shame they'll never meet.\n",
            "\n",
            "3.  Why did the scarecrow win an award?\n",
            "    Because he was outstanding in his field!\n",
            "\n",
            "4.  What do you call a lazy kangaroo?\n",
            "    Pouch potato!\n",
            "\n",
            "5.  Why did the bicycle fall over?\n",
            "    Because it was two tired!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "    async for event in result.stream_events():\n",
        "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stream item code\n",
        "Let’s break it down piece by piece.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The `@function_tool`–decorated helper\n",
        "\n",
        "```python\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "```\n",
        "\n",
        "1. **`@function_tool` decorator**  \n",
        "   This annotation tells your agent framework, “Please expose this Python function as a callable tool.” Under the hood, the library will wrap `how_many_jokes` so that when the agent decides it needs to call a “tool” named `how_many_jokes`, it actually invokes this function and returns its result back into the agent’s reasoning loop.\n",
        "\n",
        "2. **Function signature**  \n",
        "   - **Name**: `how_many_jokes`  \n",
        "   - **Return type**: `int` (as hinted by the Python type annotation `-> int`)\n",
        "\n",
        "3. **Function body**  \n",
        "   ```python\n",
        "   return random.randint(1, 10)\n",
        "   ```\n",
        "   - Uses Python’s standard `random` module to pick an integer between 1 and 10, inclusive.\n",
        "   - Every time the agent calls this tool, it’ll get back a fresh random number in that range—useful, for example, if your agent wants to decide “How many jokes should I tell?” dynamically.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Advanced event‐loop processing\n",
        "\n",
        "```python\n",
        "async for event in result.stream_events():\n",
        "    # We'll ignore the raw responses event deltas\n",
        "    if event.type == \"raw_response_event\":\n",
        "        continue\n",
        "\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "        continue\n",
        "\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        if event.item.type == \"tool_call_item\":\n",
        "            print(\"-- Tool was called\")\n",
        "        elif event.item.type == \"tool_call_output_item\":\n",
        "            print(f\"-- Tool output: {event.item.output}\")\n",
        "        elif event.item.type == \"message_output_item\":\n",
        "            print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "        else:\n",
        "            pass  # Ignore other event types\n",
        "```\n",
        "\n",
        "Here you’re iterating over **all** of the streaming events that the agent emits, not just the text tokens. Let’s unpack each branch:\n",
        "\n",
        "1. **`async for event in result.stream_events()`**  \n",
        "   - As before, you’re listening asynchronously to a firehose of events emitted during the run.\n",
        "\n",
        "2. **Ignoring “raw” text deltas**  \n",
        "   ```python\n",
        "   if event.type == \"raw_response_event\":\n",
        "       continue\n",
        "   ```\n",
        "   - These are the low-level token fragments (`ResponseTextDeltaEvent`) we saw earlier. Here you’ve decided you don’t care about printing each chunk of text directly, so you skip them.\n",
        "\n",
        "3. **Agent‐updated events**  \n",
        "   ```python\n",
        "   elif event.type == \"agent_updated_stream_event\":\n",
        "       print(f\"Agent updated: {event.new_agent.name}\")\n",
        "       continue\n",
        "   ```\n",
        "   - Sometimes the framework will swap in a new “agent” mid‐run (for example, if your agent calls a sub‐agent or changes strategy).  \n",
        "   - When that happens, you get an `agent_updated_stream_event`, and `event.new_agent` holds the fresh agent object. You simply log its name.\n",
        "\n",
        "4. **Run‐item events**  \n",
        "   ```python\n",
        "   elif event.type == \"run_item_stream_event\":\n",
        "       ...\n",
        "   ```\n",
        "   - A “run item” can be a tool invocation, the tool’s result, or an ordinary chat message. You inspect `event.item.type` to distinguish:\n",
        "\n",
        "   - **`tool_call_item`**  \n",
        "     ```python\n",
        "     print(\"-- Tool was called\")\n",
        "     ```\n",
        "     Fired when the agent decides “I’m going to call a tool now.” No output yet, just the invocation.\n",
        "\n",
        "   - **`tool_call_output_item`**  \n",
        "     ```python\n",
        "     print(f\"-- Tool output: {event.item.output}\")\n",
        "     ```\n",
        "     After the tool runs, you get this event along with `event.item.output`, the actual return value (e.g. the integer from `how_many_jokes`).\n",
        "\n",
        "   - **`message_output_item`**  \n",
        "     ```python\n",
        "     print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "     ```\n",
        "     When the agent emits a normal chat‐style response message (e.g. “Here are your jokes…”), it’s packaged as a `message_output_item`. You use `ItemHelpers.text_message_output(...)` to extract the full text from that item and print it.\n",
        "\n",
        "   - **Other items**  \n",
        "     There are a few other event/item types (tool‐error, metadata, etc.). You’ve chosen to ignore them with `pass`.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Why `asyncio.run(main())`?\n",
        "\n",
        "At the bottom of many Python async scripts, you’ll often see:\n",
        "\n",
        "```python\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "- **What it does**  \n",
        "  - `main()` is your top‐level `async def` function that kicks off everything (creates the agent, runs the prompt, loops over events, etc.).  \n",
        "  - `asyncio.run(...)` creates an event loop, schedules `main()` on it, runs until `main()` completes, then closes the loop cleanly.\n",
        "\n",
        "- **Why you need it**  \n",
        "  In a script (outside of an interactive REPL or notebook), you can’t just call `await main()` at the top level. You must bootstrap the async runtime yourself—and `asyncio.run` is the recommended way in Python 3.7+.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pikemA8-SV0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "\n",
        "    )\n",
        "    print(\"=== Run starting ===\")\n",
        "    async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "print(\"=== Run complete ===\")"
      ],
      "metadata": {
        "id": "pTNf1noxCRi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3ad69c-9a2f-4b3a-e053-6f6d20c2e5be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run starting ===\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 8\n",
            "-- Message output:\n",
            " Ok, I will tell you 8 jokes.\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "Because they make up everything!\n",
            "\n",
            "What do you call a lazy kangaroo?\n",
            "Pouch potato!\n",
            "\n",
            "What do you call a sad strawberry?\n",
            "A blueberry!\n",
            "\n",
            "Why did the bicycle fall over?\n",
            "Because it was two tired!\n",
            "\n",
            "What musical instrument is found in the bathroom?\n",
            "A tuba toothpaste.\n",
            "\n",
            "Why did the teddy bear say no to dessert?\n",
            "Because she was stuffed.\n",
            "\n",
            "Knock, knock!\n",
            "Who's there?\n",
            "Lettuce.\n",
            "Lettuce who?\n",
            "Lettuce in, it's cold out here!\n",
            "\n",
            "Why did the scarecrow win an award?\n",
            "Because he was outstanding in his field!\n",
            "\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    }
  ]
}